Worklog
===

* Spent 3 hours getting the crawler project setup and developing
  an MVP version.
* Spent 4 hours iterating on the best way to collect stats and evaluate the
  scraping process. Considered custom scrapy stats, custom stats done manually,
  using a third-party solution for live metrics (like Influx + Grafana). Finally
  decided on augmenting collected data points with enough information to generate
  statistics at the end of the process. Implemented stats collection and evaluation of
  scraping process. Created dashboard for evaluation results. Planning for improveing the numbers (speed, accuraccy etc.)
* Spent 2 hours for deployment and improvements on scraping.
* Spent 2 more hours for improving scraping and curating data.
* Spent 4 hours importing everything into ElasticSearch, writing the API, evaluating and writing the report.

**Total hours: 15**
