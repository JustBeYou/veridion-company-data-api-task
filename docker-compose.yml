services:
  # Initialize volume with proper permissions
  volume-init:
    image: alpine:latest
    volumes:
      - crawler_data:/data
    command: >
      sh -c "
        chown -R 1000:1000 /data &&
        chmod -R 755 /data &&
        echo 'Volume permissions set successfully'
      "
    user: root

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.2
    environment:
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - bootstrap.memory_lock=true
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
    ports:
      - 9200:9200
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:9.0.2
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - XPACK_SECURITY_ENABLED=false
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY="something_at_least_32_characters_long"
      - XPACK_REPORTING_ENCRYPTIONKEY="something_at_least_32_characters_long"
      - XPACK_SECURITY_ENCRYPTIONKEY="something_at_least_32_characters_long"
    ports:
      - 5601:5601
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  crawler:
    build:
      context: ./crawler
      dockerfile: Dockerfile.crawler
    volumes:
      - crawler_data:/app/data
      - ./crawler/configs:/app/configs:ro
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - CRAWLER_SLEEP_MINUTES=60
      - DOMAIN_LIMIT=10000
    user: "1000:1000"
    depends_on:
      volume-init:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_healthy
    command: [ "./run_crawler_cron.sh" ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import os; exit(0 if os.path.exists('/app/data') else 1)" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  dashboard:
    build:
      context: ./crawler
      dockerfile: Dockerfile.dashboard
    ports:
      - "5000:5000"
    volumes:
      - crawler_data:/app/data:ro
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
      - ES_URI=http://elasticsearch:9200
    user: "1000:1000"
    depends_on:
      crawler:
        condition: service_started
      volume-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  crawler_data:
    driver: local
  elasticsearch_data:
    driver: local

networks:
  default:
    name: crawler_network
