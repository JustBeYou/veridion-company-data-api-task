services:
  # Initialize volume with proper permissions
  volume-init:
    image: alpine:latest
    volumes:
      - crawler_data:/data
    command: >
      sh -c "
        chown -R 1000:1000 /data &&
        chmod -R 755 /data &&
        echo 'Volume permissions set successfully'
      "
    user: root

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.2
    environment: [ 'ES_JAVA_OPTS=-Xms2g -Xmx2g', 'bootstrap.memory_lock=true', 'discovery.type=single-node', 'xpack.security.enabled=false', 'xpack.security.enrollment.enabled=false' ]
    ports:
      - 9200:9200
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  crawler:
    build:
      context: ./crawler
      dockerfile: Dockerfile.crawler
    volumes:
      - crawler_data:/app/data
      - ./crawler/configs:/app/configs:ro
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - CRAWLER_SLEEP_MINUTES=60
      - DOMAIN_LIMIT=5
    user: "1000:1000"
    depends_on:
      volume-init:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_healthy
    command: [ "./run_crawler_cron.sh" ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import os; exit(0 if os.path.exists('/app/data') else 1)" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  dashboard:
    build:
      context: ./crawler
      dockerfile: Dockerfile.dashboard
    ports:
      - "5000:5000"
    volumes:
      - crawler_data:/app/data:ro
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
    user: "1000:1000"
    depends_on:
      crawler:
        condition: service_started
      volume-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  crawler_data:
    driver: local
  elasticsearch_data:
    driver: local

networks:
  default:
    name: crawler_network
